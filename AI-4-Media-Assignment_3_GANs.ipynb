{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of DCGAN-Keras-Cutsom-Dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1CXqrESDzz5"
      },
      "source": [
        "# DCGAN - Custom Dataset\n",
        "## Deep Convolutional GAN\n",
        "\n",
        "This is much the same as the previous notebook we looked at (still using the [official Keras DCGAN implementation](https://keras.io/examples/generative/dcgan_overriding_train_step/)) but I have some extra code at the start to allow you to make your own dataset from a YouTube video!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcXXG7NMcHAl"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import gdown\n",
        "from zipfile import ZipFile\n",
        "import cv2\n",
        "import math\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CotjyJ1-ewMz"
      },
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFQoTc66Mi4f"
      },
      "source": [
        "We have to install [YouTube-DL](https://youtube-dl.org/) via pip in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMkxqCTQif06"
      },
      "source": [
        "!pip install --upgrade --quiet youtube_dl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSx3LqLKMveN"
      },
      "source": [
        "All you need to do is to run the cell below. It just contains helper functions to download a YouTube video and to extract frames from the video to make the dataset. Feel free to poke around if you're interested."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCo8qlByilMl"
      },
      "source": [
        "# Some helper functions we will use to make the dataset\n",
        "\n",
        "from __future__ import unicode_literals\n",
        "import youtube_dl\n",
        "\n",
        "\n",
        "class MyLogger(object):\n",
        "    def debug(self, msg):\n",
        "        pass\n",
        "\n",
        "    def warning(self, msg):\n",
        "        pass\n",
        "\n",
        "    def error(self, msg):\n",
        "        print(msg)\n",
        "\n",
        "\n",
        "def my_hook(d):\n",
        "    if d['status'] == 'finished':\n",
        "        print('Done downloading.')\n",
        "\n",
        "def download_youtube_video(_url):\n",
        "  ydl_opts = {\n",
        "      'format': '(mp4)[height>=256][height<=400]',\n",
        "      'outtmpl': '%(id)s.%(ext)s',\n",
        "      'logger': MyLogger(),\n",
        "      'progress_hooks': [my_hook],\n",
        "  }\n",
        "  with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
        "      result = ydl.extract_info(_url, download=True)\n",
        "\n",
        "  if 'entries' in result:\n",
        "    video = result['entries'][0]\n",
        "  else:\n",
        "    video = result\n",
        "\n",
        "  return video\n",
        "\n",
        "def analyse_video(_videoPath):\n",
        "  vidcap = cv2.VideoCapture(_videoPath)\n",
        "  success, frame = vidcap.read()\n",
        "\n",
        "  frameCount = 0\n",
        "  darkFrames = []\n",
        "  validFrames = []\n",
        "\n",
        "  while success:\n",
        "    grey = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    average = grey.mean(axis=0).mean(axis=0)\n",
        "\n",
        "    if average < 2:\n",
        "      darkFrames.append(frameCount)\n",
        "    else:\n",
        "      validFrames.append(frameCount)\n",
        "    \n",
        "    success, frame = vidcap.read()\n",
        "    frameCount += 1\n",
        "  \n",
        "  print(f'Found {len(darkFrames)} dark frames.')\n",
        "  return validFrames, darkFrames\n",
        "\n",
        "def extract_frames(_videoPath, _outputPath, _num, _size):\n",
        "  SIZE = _size[0]\n",
        "  MAX = _num\n",
        "  count = 0\n",
        "  id = 0\n",
        "  validFrames, darkFrames = analyse_video(_videoPath)\n",
        "  doubles = []\n",
        "  frames = []\n",
        "\n",
        "  if MAX > len(validFrames):\n",
        "    numDoubles = MAX - len(validFrames)\n",
        "    doubles = np.random.choice(validFrames, size=numDoubles, replace=False)\n",
        "    frames = validFrames\n",
        "  else:\n",
        "    frames = np.random.choice(validFrames, size=MAX, replace=False)\n",
        "\n",
        "  vidcap = cv2.VideoCapture(_videoPath)\n",
        "  success, frame = vidcap.read()\n",
        "\n",
        "  frameHeight = frame.shape[0]\n",
        "  frameWidth = frame.shape[1]\n",
        "\n",
        "  scaleFactor = SIZE / frameHeight\n",
        "  newWidth = int(frameWidth * scaleFactor)\n",
        "  padding = int((newWidth - SIZE) / 2)\n",
        "\n",
        "  while success:\n",
        "    if count in frames:\n",
        "      frame = cv2.resize(frame, (newWidth, SIZE), interpolation=cv2.INTER_AREA)\n",
        "      crops = []\n",
        "\n",
        "      if count in doubles:\n",
        "        crops = [frame[0:SIZE, 0:SIZE],\n",
        "                frame[0:SIZE, padding*2:SIZE+padding*2]]\n",
        "      else:\n",
        "        crops = [frame[0:SIZE, padding:SIZE+padding]]\n",
        "\n",
        "      for crop in crops:\n",
        "        try:\n",
        "          cv2.imwrite(os.path.join(_outputPath, f'{id:04}.jpg'), crop)\n",
        "          id += 1\n",
        "        except:\n",
        "          print(\"Error saving frame.\")\n",
        "          pass\n",
        "    \n",
        "    count += 1\n",
        "    success, frame = vidcap.read()\n",
        "\n",
        "  print(f\"Saved {id} images from video '{videoInfo['title']}'\")\n",
        "\n",
        "  return id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbvkuYrrKMZ8"
      },
      "source": [
        "# Choose a YouTube Video\n",
        "\n",
        "Find a video on YouTube which is about 4-10 minutes long. The video can be anything but ideally it will be generally consistent throughout. So timelapse video are perfect, [like this video of clouds forming](https://www.youtube.com/watch?v=NJfI_GaEyJw), or [this video of life underwater](https://www.youtube.com/watch?v=J2BKd5e15Jc) has nice consistent colours and forms. However it is entirely up to you, maybe it would be interesting to get a random video of Lady Gaga dresses.. who knows!\n",
        "\n",
        "Paste the YouTube video URL in the cell below, __replacing the url that is between the single quotes__.\n",
        "\n",
        "If the URL is long like this:\n",
        "\n",
        "```\n",
        "https://www.youtube.com/watch?v=NJfI_GaEyJw&ab_channel=wizard327\n",
        "                                           ^\n",
        "                       We don't need the stuff after this & symbol.\n",
        "```\n",
        "\n",
        "just trim the end off after (and including) the '__&__' symbol."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoIgi2xYisel"
      },
      "source": [
        "url = 'https://www.youtube.com/watch?v=J2BKd5e15Jc' # Coral Reef\n",
        "videoInfo = download_youtube_video(url)\n",
        "videoFile = \"{0}.{1}\".format(videoInfo['webpage_url'].split('=')[-1], videoInfo['ext'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_-OFQLWL-U8"
      },
      "source": [
        "If your video has an intro and an outro we can trim that off! Change the `startTime` and `endTime` values below. The values need to be in seconds, so if the good bit of the video start at 38 seconds then enter `38` for `startTime`. If the end credits start at 9:14, then an easy way to find the seconds is `(9*60)+14`.\n",
        "\n",
        "__If you don't need to trim the video down then just don't run the cell below.__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m61KiSm-iugQ"
      },
      "source": [
        "# Optional trimming of the video to remove intro / end credits.\n",
        "# Skip this if your video does not need trimming.\n",
        "\n",
        "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\n",
        "\n",
        "def trim_video(_video, _start, _end):\n",
        "  trimmedVideo = f\"trimmed.{videoInfo['ext']}\"\n",
        "  ffmpeg_extract_subclip(videoFile, _start, _end, targetname=trimmedVideo)\n",
        "  return trimmedVideo\n",
        "\n",
        "\n",
        "startTime = 38\n",
        "endTime = (9*60)+14\n",
        "videoFile = trim_video(videoFile, startTime, endTime)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWdv0dpWN7Gh"
      },
      "source": [
        "Next we make some directories and extract 3000 images from the the video."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRHTno-9cMG4"
      },
      "source": [
        "DATASET_DIR = 'dataset'\n",
        "IMAGES_DIR = f\"{DATASET_DIR}/images\"\n",
        "OUTPUT_DIR = 'output'\n",
        "IMAGE_SIZE =(64, 64)\n",
        "try:\n",
        "  os.makedirs(DATASET_DIR)\n",
        "  os.makedirs(IMAGES_DIR)\n",
        "  os.makedirs(OUTPUT_DIR)\n",
        "except:\n",
        "  pass\n",
        "NUM_IMAGES = extract_frames(f\"/content/{videoFile}\", IMAGES_DIR, 3000, IMAGE_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcVHKvapOCSa"
      },
      "source": [
        "As with before we then turn it into a _Tensorflow dataset_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEpCzNfCc0_7"
      },
      "source": [
        "dataset = keras.preprocessing.image_dataset_from_directory(\n",
        "    DATASET_DIR, label_mode=None, image_size=IMAGE_SIZE, batch_size=32\n",
        ")\n",
        "dataset = dataset.map(lambda x: x / 255.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28RcFXw8OGsY"
      },
      "source": [
        "All things going well, if you run the cell below you should see frames from the video you chose being taken from the dataset we created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNOYzTQac1od"
      },
      "source": [
        "for x in dataset:\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow((x.numpy() * 255).astype(\"int32\")[0])\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ahot01TYOOKW"
      },
      "source": [
        "# Defining the Model\n",
        "\n",
        "This is the same as the previous example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPjWwh8CdeK8"
      },
      "source": [
        "discriminator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(64, 64, 3)),\n",
        "        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(1, activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"discriminator\",\n",
        ")\n",
        "discriminator.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ve5WyO5vdhg7"
      },
      "source": [
        "latent_dim = 128\n",
        "\n",
        "generator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(latent_dim,)),\n",
        "        layers.Dense(8 * 8 * 128),\n",
        "        layers.Reshape((8, 8, 128)),\n",
        "        layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(3, kernel_size=5, padding=\"same\", activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"generator\",\n",
        ")\n",
        "generator.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28BVDjCNdnT4"
      },
      "source": [
        "class GAN(keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim):\n",
        "        super(GAN, self).__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super(GAN, self).compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
        "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.d_loss_metric, self.g_loss_metric]\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        # Sample random points in the latent space\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Decode them to fake images\n",
        "        generated_images = self.generator(random_latent_vectors)\n",
        "\n",
        "        # Combine them with real images\n",
        "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
        "\n",
        "        # Assemble labels discriminating real from fake images\n",
        "        labels = tf.concat(\n",
        "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
        "        )\n",
        "        # Add random noise to the labels - important trick!\n",
        "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
        "\n",
        "        # Train the discriminator\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combined_images)\n",
        "            d_loss = self.loss_fn(labels, predictions)\n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(\n",
        "            zip(grads, self.discriminator.trainable_weights)\n",
        "        )\n",
        "\n",
        "        # Sample random points in the latent space\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Assemble labels that say \"all real images\"\n",
        "        misleading_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "        # Train the generator (note that we should *not* update the weights\n",
        "        # of the discriminator)!\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
        "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "\n",
        "        # Update metrics\n",
        "        self.d_loss_metric.update_state(d_loss)\n",
        "        self.g_loss_metric.update_state(g_loss)\n",
        "        return {\n",
        "            \"d_loss\": self.d_loss_metric.result(),\n",
        "            \"g_loss\": self.g_loss_metric.result(),\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlygRVfXdrCK"
      },
      "source": [
        "class GANMonitor(keras.callbacks.Callback):\n",
        "    def __init__(self, num_img=1, latent_dim=128):\n",
        "        self.num_img = num_img\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
        "        generated_images = self.model.generator(random_latent_vectors)\n",
        "        generated_images *= 255\n",
        "        generated_images.numpy()\n",
        "        for i in range(self.num_img):\n",
        "            img = keras.preprocessing.image.array_to_img(generated_images[i])\n",
        "            img.save(os.path.join(OUTPUT_DIR, \"generated_img_%03d_%d.png\" % (epoch, i)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilBtgRC2OS6E"
      },
      "source": [
        "# Train the GAN\n",
        "\n",
        "Now our dataset is much much smaller than [CelebA](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) and so this will train a lot quicker.\n",
        "\n",
        "Unfortunately Keras likes to output a lot of it's own information during training which makes it difficult to display images using `imshow` or something similar. But as with before images from the generator are saved to a directory called `output` which you can find in the file explorer on the left."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjCwDPALdtgA"
      },
      "source": [
        "epochs = 60\n",
        "\n",
        "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
        "gan.compile(\n",
        "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "    g_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "    loss_fn=keras.losses.BinaryCrossentropy(),\n",
        ")\n",
        "\n",
        "gan.fit(\n",
        "    dataset, epochs=epochs, callbacks=[GANMonitor(num_img=1, latent_dim=latent_dim)]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0tPBlhsI5vm"
      },
      "source": [
        "# Extra: Saving a Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RFwq3AyHiAW"
      },
      "source": [
        "# Make a new directory to save the model into\n",
        "os.mkdir('model')\n",
        "gan.generator.save('model') # We're just saving the generator here as that's the interesting bit!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTQ8KHFIHzG5"
      },
      "source": [
        "# Give Colab access to your Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9tFTa7OH8oj"
      },
      "source": [
        "# Copy the 'model' folder to your drive\n",
        "# -r means 'copy recursively' so it copies all the files and folders\n",
        "# inside the 'model' directory.\n",
        "!cp -r /content/model/ /content/drive/MyDrive/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ek-2hBQvb0gc"
      },
      "source": [
        "# Look at outputs for each epoch (Step 3)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9YaUMm8XSrb"
      },
      "source": [
        "#Get image paths\n",
        "my_images = []\n",
        "for root, dirs, files in os.walk(OUTPUT_DIR, topdown=False):\n",
        "    for name in files:\n",
        "        if not \".DS_Store\" in name:\n",
        "            my_images.append(os.path.join(root, name))\n",
        "    my_images =np.array(my_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaYa1FiYXHr1"
      },
      "source": [
        "#Show images\n",
        "num_to_show = len(my_images)\n",
        "plt.figure(figsize=(12,12))\n",
        "cols = 10\n",
        "rows = int(np.ceil(num_to_show / cols))\n",
        "for index, im in enumerate(my_images):\n",
        "    loaded_image = cv2.cvtColor(cv2.resize(cv2.imread(im), IMAGE_SIZE), cv2.COLOR_BGR2RGB)\n",
        "    plt.subplot(rows, cols, index+1)\n",
        "    plt.imshow(loaded_image, interpolation=\"bilinear\")\n",
        "    plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNr75JmAb6vF"
      },
      "source": [
        "# Interpolate between two points (Step 4)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laPp6EowZas0"
      },
      "source": [
        "#editted from https://machinelearningmastery.com/how-to-interpolate-and-perform-vector-arithmetic-with-faces-using-a-generative-adversarial-network/\n",
        "from numpy import asarray\n",
        "from numpy.random import randn\n",
        "from numpy.random import randint\n",
        "from numpy import linspace\n",
        "from keras.models import load_model\n",
        "\n",
        " # generate points in latent space as input for the generator\n",
        "def generate_latent_points(latent_dim, n_samples, n_classes=10):\n",
        "  # generate points in the latent space\n",
        "  x_input = randn(latent_dim * n_samples)\n",
        "  # reshape into a batch of inputs for the network\n",
        "  z_input = x_input.reshape(n_samples, latent_dim)\n",
        "  return z_input\n",
        "\n",
        "# uniform interpolation between two points in latent space\n",
        "def interpolate_points(p1, p2, n_steps=10):\n",
        "  # interpolate ratios between the points\n",
        "  ratios = linspace(0, 1, num=n_steps)\n",
        "  # linear interpolate vectors\n",
        "  vectors = list()\n",
        "  for ratio in ratios:\n",
        "    v = (1.0 - ratio) * p1 + ratio * p2\n",
        "    vectors.append(v)\n",
        "  return asarray(vectors)\n",
        "\n",
        "# create a plot of generated images\n",
        "def plot_generated(examples):\n",
        "  # plot images\n",
        "  num_to_show = len(examples)\n",
        "  plt.figure(figsize=(12,12))\n",
        "  cols = 3\n",
        "  rows = int(np.ceil(num_to_show / cols))\n",
        "  for i in range(num_to_show):\n",
        "    # define subplot\n",
        "    plt.subplot(rows, cols, i+1)\n",
        "    # turn off axis\n",
        "    plt.axis('off')\n",
        "    # plot raw pixel data\n",
        "    plt.imshow(examples[i])\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUySTsLIqVoD"
      },
      "source": [
        "### How many images do you want?\n",
        "\n",
        "Pick the number of images to interpolate. It will pick two new random points to interpolate between every time you run it! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tM9Qi_iDXib8"
      },
      "source": [
        "num_images = 20\n",
        "\n",
        "# generate points in latent space\n",
        "pts = generate_latent_points(latent_dim, 2)\n",
        "print(pts.shape)\n",
        "# interpolate points in latent space\n",
        "interpolated = interpolate_points(pts[0], pts[1], num_images)\n",
        "# generate images\n",
        "X = gan.generator(interpolated)\n",
        "X *= 255\n",
        "X.numpy()\n",
        "generated_images = []\n",
        "for i in range(num_images):\n",
        "    generated_images.append(keras.preprocessing.image.array_to_img(X[i]))\n",
        "plt.figure(figsize=(12,12))\n",
        "plot_generated(generated_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pk19nt-ZQkj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}