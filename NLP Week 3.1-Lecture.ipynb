{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Notebook for Week 3 lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cosine\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from nltk.tokenize.casual import casual_tokenize\n",
    "import pandas as pd\n",
    "from nlpia.data.loaders import get_data\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Example: Using tf-idf to do a search\n",
    "\n",
    "### Review: Starting with documents, compute some tfidf vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's make some documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "s1 = \"Samsung Galaxy S7 32GB - Black - Unlocked (Renewed) 5.1-inch touchscreen display with a resolution of 1440 pixels by 2560 pixels at a PPI of 577 pixels per inch Samsung Exynos 8890 Octa-core 64-bit processor12-megapixel primary camera on the rear and a 5-megapixel front camera 32GB of internal storage can be expanded (MicroSD)\"\n",
    "s2 = \"Samsung Smartphone Galaxy S8 64GB UK Version - Midnight Black 5.8” QuadHD + sAMOLED display – Rounded-corner Infinity Display and symmetrical metal sides that blend effortlessly for a seamless look(unlocked) IP68-rated water and dust resistant with a powerful 10nm mobile AP for multi-tasking Dual pixel 12MP camera with F1.7 lens and enhanced image processing. 8MP front camera with facial recognition for smart autofocus 3,000mAh battery with fast charging capabilities via USB Type-C. Wireless Charger Convertible Expandable memory up to 256GB for storing all your photos, movies and music\"\n",
    "s3 = \"JOYIN Play-act Pretend Play Smart Phone, Keyfob Key Toy and Credit Cards Set Kids Toddler Cellphone Key Toys Great Value. Set includes Electronic Toy Keyfob, Electronic Toy Phone, Driver's License and Debit Card. Each Toy Accessory Provides Different Play Patterns Adding-up to Endless Hands-on Playtime. SOUND EFFECTS. Toy Mobile Phone (Requires 3 AAA Batteries Not Included) Talks Back to Kids with14 Different Unique Phrases and Music When Touched. Electronic Toy Keyfob (Requires 3 L44 Batteries Included) Features Colorful Press Buttons with Three Different Car Sounds INCREDIBLE DETAILS. Driver License and Debit Cards are all Designed with Details and Fashion.It’s Handy and Perfect for Toddler Fashionistas to Play PREMIUM QUALITY & SAFETY. Child Safe: Non-Toxic. Meet US toy standard. Safety test approved. CUSTOMER SATISFACTION. Providing a 100% satisfaction experience is our main priority to our customers. Feel free to message us through “contact sellers” if products don't meet your expectations. The celebrations start at JOYIN!\"\n",
    "s4 = \"Ricco Kids TWO MOTORS Battery Powered i8 Style Sports Coupe Electric Ride On Toy Car (Model: KL1888) (WHITE) Electric Ride On Car with Excellent Quality and Comfort, Parental Remote Control Included Two 6V 4.5AH Batteries and Two 20W Motors, Four Wheels Suspension, LED Light, Horn, Power Level Display MP3 Music Input Interface on steering wheel, Foot Pedal Accelerator, Forward, reverse and neutral gears. Designed for Age: 3-6 Years Old, Max Capacity: 30 KGS, Max Speed: 5 KM/Hour Product Size: 102*68*44 CM, Net Weight: 13 KGS, part assembled, Charging Time:8~12 hours, Driving Time: about 45 minutes. Standards complied: GB6675 GB19865 EN71\\EN62115 ASTM-F963\"\n",
    "\n",
    "documents = [s1, s2, s3, s4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's use our own tokenizer for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Louis' tokenizer from last week\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "def my_tokeniser(doc):\n",
    "    #Split on spaces\n",
    "    tokens = re.split(r'[-\\s.,;!?]+', doc)\n",
    "    processed = []\n",
    "    for t in tokens:\n",
    "        #Lemmatise and make lowercase\n",
    "        t = lem.lemmatize(t.lower()) #Can try changing this line to see how it impacts results (e.g., remove lemmatising, keep lowercase)\n",
    "        #Remove stop words\n",
    "        if not t in stop_words.ENGLISH_STOP_WORDS:\n",
    "            processed = processed + [t]\n",
    "    #Return an array of tokens for that document\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's compute the tfidf vectors for each document in our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Create count vectors\n",
    "cv = CountVectorizer(min_df=1, tokenizer=my_tokeniser)\n",
    "count_array = cv.fit_transform(documents).toarray()\n",
    "\n",
    "#create tfidf vectors from count vectors\n",
    "transformer = TfidfTransformer()\n",
    "transformer.fit(count_array)\n",
    "tfidfs = transformer.transform(count_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "It's helpful to store our tfidfs in a pandas dataframe, which is easy to display and workwith. Note though that the representation the transformer gives us is a 'compressed sparse row' matrix, which means it's stored in a compact representation. We need to convert it to a dense representation (i.e., the sort of matrix you're used to seeing) to create a dataframe from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vocab = cv.get_feature_names()\n",
    "data = pd.DataFrame(tfidfs.todense(), columns = vocab) #store it in a nice data frame to make it easy to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data #just displays our tfidf data as a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#More helpful to print top 10 words for each document (i.e., words with top tfidf values)\n",
    "num_words = 10\n",
    "for i in range(len(tfidfs.todense())):\n",
    "    print(\"doc\", i)\n",
    "    print(data.iloc[i].sort_values(ascending = False).head(num_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's compute cosine similarities between all pairs of documents using this tfidf representation, like Louis showed last week:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Convert to array \n",
    "tfidfs_dense = tfidfs.todense()\n",
    "#Find similarities\n",
    "result = cosine(tfidfs_dense)\n",
    "#Put the result in a dataframe \n",
    "df = pd.DataFrame(result)\n",
    "#Show with heatmap style gradients\n",
    "df.style.background_gradient(cmap='Greens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Using tfidf vectors to query\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "query = [\"phone\"] #Replace this with whatever word(s) you want to use as your query\n",
    "query_vector = cv.transform(query) #produce our count vectors, using the same method (including tokenizer) as we used on the corpus\n",
    "query_tfidf = transformer.transform(query_vector) #produce tfidf scores for this query, using the same method as we used on the corpus\n",
    "\n",
    "#query_tfidf is now a sparse (CSR) matrix; we need to get the \"dense\" version to compare it to our other tfidf vectors\n",
    "dense_query = query_tfidf.todense()\n",
    "\n",
    "#This will compute cosine similarity between dense_query and each element of the vector array\n",
    "#The first element is the similarity with document 1, the second is the similarity with document 2, and so on\n",
    "cosine(dense_query, tfidfs_dense)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Latent Semantic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To explore LSA, we'll use a dataset of SMS messages -- some spam, others not -- from the NLPIA book. To get this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sms = get_data('sms-spam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Take a look:\n",
    "sms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Before we make a dataframe, let's make a fancy index that gives each sms a name, which ends in ! if it's spam:\n",
    "index = ['sms{}{}'.format(i, '!'*j) for (i, j) in zip(range(len(sms)), sms.spam)]\n",
    "\n",
    "#now let's plop our messages into a dataframe, where this new index array becomes the dataframe index array\n",
    "# each row in the dataframe is a text message. the \"text\" column holds the text we actually want to analyse\n",
    "sms = pd.DataFrame(sms.values, columns=sms.columns, index=index)\n",
    "sms #print it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As an alterantive to the above, where we used a counter with a custom tokenizer, followed by a TfidfTransformer, we can make a TfidfVectorizer that uses an existing tokenizer. Here, casual_tokenize (from nltk toolkit) is good for tokenizing text like sms where the language may be casual, we may have emojis, etc. (https://www.nltk.org/_modules/nltk/tokenize/casual.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectoriser = TfidfVectorizer(tokenizer=casual_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Note that the documentation for TfidfVectorizer says it's \"Equivalent to :class:`CountVectorizer` followed by\n",
    ":class:`TfidfTransformer`.\" :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "?TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can now use `tfidf_vectoriser.fit_transform` to transform the raw documents using this call; it returns a sparse matrix which we can convert into a normal matrix using `.toarray()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf_docs = tfidf_vectoriser.fit_transform(raw_documents=sms.text).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here's some code that lets us explore the tfidf outputs a bit, doing some sanity checking before proceeding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "len(tfidf_vectoriser.vocabulary_) #shows us size of the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "shape(tfidf_docs) #The rows and columns in our array (1 row per document, 1 column per term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We'd like to make a dataframe from `tfidf_docs` for display/processing convenience, but we want to know which columns correspond to which terms. We do this complicated zip operation to get a list of column_names and a list of corresponding terms (in the same order). We can then use `terms` to set the column names in our data frame. Note that the row names in our data frame (i.e., the indexes) are the same as the original sms data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "column_nums, terms = zip(*sorted(zip(tfidf_vectoriser.vocabulary_.values(), tfidf_vectoriser.vocabulary_.keys()))) # Get the column_numbers for each term in our vocabulary\n",
    "tfidf_docs_df = pd.DataFrame(tfidf_docs, columns=terms, index=sms.index)\n",
    "tfidf_docs_df #show it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#It looks like sms2 has some &, ' and ( characters based on the table above. Let's verify, by viewing it in the original dataset.\n",
    "#the iloc() function gives us the dataframe data corresponding to the integer index(es) of its argument(s).\n",
    "# Here, let's grab all the columns (\":\") from row 2:\n",
    "sms.iloc[2,:].text   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Apply LSA using TruncatedSVD\n",
    "\n",
    "The first thing we need to do is subtract the mean of each tfidf column from each value (sometimes called \"whitening\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tfidf_docs_df = tfidf_docs_df - tfidf_docs_df.mean() #Centres vectorized documents by subtracting the mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we can apply TruncatedSVD to the mean-subtracted values, using fit_transform. As before, this will give us a sparse matrix. Note that this may take a little while to compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components = 16, n_iter = 1000)\n",
    "svd_topic_vectors = svd.fit_transform(tfidf_docs_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#put it in a dataframe, again being good to ourselves by giving it row and column names\n",
    "svd_topic_vectors_df = pd.DataFrame(svd_topic_vectors, index=sms.index, columns=['topic{}'.format(i) for i in range(16)])\n",
    "svd_topic_vectors_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Notice from the above that we've managed to represent each SMS message in just 16 numbers! :D wow!\n",
    "\n",
    "But there's more!\n",
    "\n",
    "We can look at the weights LSA has assigned to each word within each topic. `svd.components_` is a variable that gives us these weightings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "shape(svd.components_) #it has 16 rows (one per topic) and 9232 columns (one per word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can also peek at the singular values using `svd.singular_values_` if we're interested (usually we're not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "svd.singular_values_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#let's stick this in a friendly dataframe\n",
    "#below, we use .T to transpose svd.components_ into the familiar form where *rows* correspond to words and *columns* to topics\n",
    "topic_weights = pd.DataFrame(svd.components_.T, index=terms, columns=['topic{}'.format(i) for i in range(16)])\n",
    "topic_weights #display it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can examine our topics by looking at which words are highly weighted in each topic. To do this for an individual topic, we can use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "topic_weights.topic2.sort_values(ascending=False)[:10] # show top 10 weighted words for topic 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Or we can do this for every topic, using a for-loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Do this for all topics\n",
    "for i in range(16):\n",
    "    print(\"topic \" + str(i) + \":\")\n",
    "    topicName = \"topic\" + str(i)\n",
    "    weightedlist = topic_weights.get(topicName).sort_values(ascending=False)[:10]\n",
    "    print(weightedlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can also see the topic spread for each word in a new fake sms that we construct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_columns=16 #Just to make sure we can see everything\n",
    "weights_for_new_SMS = topic_weights.T['! ;) :) half off crazy deal discount'.split()].round(3) * 100\n",
    "weights_for_new_SMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above just shows the topic weights for each individual token in the fake new SMS. Can you figure out how to construct a query vector from this SMS -- i.e., a single vector that you could compare with other messages using cosine similarity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Now let's try Latent Dirichlet Allocation on the same dataset\n",
    "\n",
    "Don't forget, we apply LDiA to the word count vectors, **not to the tf-idf vectors**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Set things up\n",
    "lda_cv = CountVectorizer(stop_words='english', tokenizer=casual_tokenize,\n",
    "                        max_df=.1,\n",
    "                        max_features=5000)\n",
    "count_data = lda_cv.fit_transform(sms.text)\n",
    "lda = LatentDirichletAllocation(n_components=16,\n",
    "                                random_state=123,\n",
    "                                learning_method='batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# And run LDA. This could take a loooonggggg time....\n",
    "lda_topics = lda.fit_transform(count_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Now make a friendly data frame from the topics and display it\n",
    "lda_topic_vectors_df = pd.DataFrame(lda_topics, index=sms.index, columns=['topic{}'.format(i) for i in range(16)])\n",
    "lda_topic_vectors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Print the top words for each topic\n",
    "#This uses some fancier Python than the way we printed out LDA topics above; either is fine!\n",
    "n_top_words = 10\n",
    "feature_names = lda_cv.get_feature_names()\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(\"Topic %d:\" % (topic_idx + 1))\n",
    "    print(\" \".join([feature_names[i]\n",
    "                    for i in topic.argsort()\\\n",
    "                        [:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Explore topic distributions for existing documents. \n",
    "#Think about how dense/sparse this distribution is compared to LSA topics\n",
    "doc_num = 9 #can change this\n",
    "print(\"SMS: \" + sms.text.get(doc_num))\n",
    "print(lda_topic_vectors_df.iloc[doc_num,:]) #see topic weighting for document # doc_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Compare to sparseness/density of distribution over LDA topics for the same text message\n",
    "#(remember that topics in LDiA will not correspond to topics in LSA)\n",
    "print(svd_topic_vectors_df.iloc[doc_num,:]) #see topic weighting for document # doc_num"
   ]
  }
 ],
 "metadata": {
 "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
