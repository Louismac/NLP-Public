{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Week 3 Session 1 activity\n",
    "\n",
    "## Part 1\n",
    "\n",
    "Please work through the \"NLP Week 3.1-Lecture.ipynb\" notebook from GitHub before returning here.\n",
    "\n",
    "Then, **choose whether to attempt Parts 2/3 (applying LSA and/or LDA to movie reviews) or Part 4 (getting started with a web scraper)**. (We encourage you to attempt all parts before next week, but there will only be time for one of these during class.)\n",
    "\n",
    "\n",
    "## Part 2\n",
    "\n",
    "In this section, you'll be applying LSA to a new dataset: movie reviews from IMDB.\n",
    "\n",
    "This section uses a data file constructed using code in [this github project](https://github.com/rasbt/python-machine-learning-book-2nd-edition/blob/master/code/ch08/ch08.py)\n",
    "\n",
    "**First, import the necessary libraries:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cosine\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from nltk.tokenize.casual import casual_tokenize\n",
    "import pandas as pd\n",
    "from nlpia.data.loaders import get_data\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Next, load the movie reviews.** (This requires movie_data.csv to be in the same directory as this notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_from_csv = pd.read_csv('movie_data.csv', encoding='utf-8') #Reads a CSV file into a pandas dataframe\n",
    "data_from_csv.head(3) #show us the first 3 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "shape(data_from_csv) #how many rows & columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Let's rename the review column to \"text\" so we can easily access it later\n",
    "df = data_from_csv.rename(columns={'0': 'text'})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Now, let's apply LSA.**\n",
    "\n",
    "Grab some code from the \"NLP Week 3.1-Lecture.ipynb\" notebook to do the following:\n",
    "\n",
    "* Create count vectors for your corpus, using a tokenizer of your choice (e.g., possibly Louis' custom tokenizer, or perhaps another one you think will be good for this task)\n",
    "* Create tf-idf vectors from the count vectors\n",
    "* Centre the vectorized documents by subtracting the mean\n",
    "* Use TruncatedSVD to compute the LSA topic vectors. Use 10 topics and 100 iterations to start with.\n",
    "* Explore the results.\n",
    "  * Do the topics seem to correspond to movie genres, to types of reviews, or anything else? \n",
    "  * Do the topic weights for individual reviews seem to make sense to you?\n",
    "* Try changing the number of topics to fewer (e.g., 3?) or more (e.g., 20) and explore how this changes the results.\n",
    "* Optional bonus: Can you write some code that allows you to construct a query for this corpus? It should use cosine similarity to find the movie review(s) that best match a text string you provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Your code here (feel free to add cells)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 3:  Apply Latent Dirichlet Allocation\n",
    "\n",
    "Again, copying code from the lecture notebook, apply LDiA to this data.\n",
    "\n",
    "You'll want to specifically do the following:\n",
    "* Compute count vectors for your corpus, using a tokenizer of your choice (probably the same one you used for LSA)\n",
    "* Apply LDA to the count vectors. And while you wait for it to compute, maybe go have a coffee? Or get a start on Part 4 below? .....\n",
    "* Explore the results. \n",
    "  * Do the topics seem to correspond to movie genres, to types of reviews, or anything else? \n",
    "  * Do the topic weights for individual reviews seem to make sense to you?\n",
    "  * How do these topics compare to the topics LSA found? Do they seem to be more or less coherent? \n",
    "  * How do the document distributions over topics compare to those from LSA? Do you find LDiA's distributions to be more sparse (i.e., with more topics having a near-0 probability within a given document)?\n",
    "* Try changing the number of topics and explore how this changes the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Your code here (feel free to add cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 4:  Experiment with Web Scraping\n",
    "\n",
    "\n",
    "**First,** visit a webpage that you might want to scrape (e.g., perhaps a site with things for sale, music lyrics, reviews, biographies of people, dating profiles, ... ?). Use the developer tools in your browser to identify the HTML tags, classes, and/or ids that correspond to the text you want to scrape.\n",
    "\n",
    "\n",
    "**Then, work through a tutorial for one (or both) of the following web scraping tools.**\n",
    "\n",
    "[Webscraper.io](https://webscraper.io/) is a GUI-based tool which allows you to crawl from a starting page and scrape without any coding. You'll probably want to start by watching a tutorial video, e.g., [this one](https://www.youtube.com/watch?v=n7fob_XVsbY&feature=emb_logo)\n",
    "\n",
    "Alternatively, you can use [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) to build a scraper in Python. Install it using:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You'll also want to install `requests` and `lxml`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "!pip install lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "There is a lot of good documentation for BeautifulSoup online. For instance, you might want to start with [this tutorial](https://www.digitalocean.com/community/tutorials/how-to-scrape-web-pages-with-beautiful-soup-and-python-3), though note that you probably want to type the Python code here rather than save it in a separate .py file and run it outside of Jupyter notebook.\n",
    "\n",
    "For your convenience, the final code from this tutorial is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "f = csv.writer(open('z-artist-names.csv', 'w'))\n",
    "f.writerow(['Name', 'Link'])\n",
    "\n",
    "pages = []\n",
    "\n",
    "for i in range(1, 5):\n",
    "    url = 'https://web.archive.org/web/20121007172955/https://www.nga.gov/collection/anZ' + str(i) + '.htm'\n",
    "    pages.append(url)\n",
    "\n",
    "\n",
    "for item in pages:\n",
    "    page = requests.get(item)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "    last_links = soup.find(class_='AlphaNav')\n",
    "    last_links.decompose()\n",
    "\n",
    "    artist_name_list = soup.find(class_='BodyText')\n",
    "    artist_name_list_items = artist_name_list.find_all('a')\n",
    "\n",
    "    for artist_name in artist_name_list_items:\n",
    "        names = artist_name.contents[0]\n",
    "        links = 'https://web.archive.org' + artist_name.get('href')\n",
    "\n",
    "        f.writerow([names, links])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
