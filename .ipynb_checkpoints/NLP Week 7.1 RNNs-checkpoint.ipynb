{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7.1 Recurrent Neural Networks\n",
    "\n",
    "We'll being using **word embeddings** as inputs, so if you've already downloaded them in Week 5, **DONT DO IT AGAIN**. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # Keras takes care of most of this but it likes to see Numpy arrays\n",
    "from keras.preprocessing import sequence    # A helper module to handle padding input\n",
    "from keras.models import Sequential         # The base keras Neural Network model\n",
    "from keras.layers import Dense, Dropout, Activation   # The layer objects we will pile into the model\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from nltk.tokenize.casual import casual_tokenize\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "!git clone https://github.com/Louismac/NLP-Public\n",
    "%cd NLP-Public\n",
    "#Code editted from NLPIA book ch8 https://github.com/totalgood/nlpia#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colab users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Run this cell ONLY IF you're on colab\n",
    "#This is going to download a giant file into your colab distribution\n",
    "!wget -P /root/input/ -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
    "embeddings_file = '/root/input/GoogleNews-vectors-negative300.bin.gz' # from above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This line will load the 200,000 most common words into wv, a variable of word vectors\n",
    "#Note that this will load these into memory, so if you don't have a lot of memory on your computer you may run into problems/slowness\n",
    "#embeddings_file = \"GoogleNews-vectors-negative300.bin.gz\"\n",
    "wv = KeyedVectors.load_word2vec_format(embeddings_file, binary=True, limit=200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True, limit=200000)\n",
    "def tokenize_and_vectorize(dataset):\n",
    "    vectorized_data = []\n",
    "    for sample in dataset:\n",
    "        tokens = casual_tokenize(sample)\n",
    "        sample_vecs = []\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                sample_vecs.append(word_vectors[token])\n",
    "            except KeyError:\n",
    "                pass  # No matching token in the Google w2v vocab\n",
    "        vectorized_data.append(sample_vecs)\n",
    "\n",
    "    return vectorized_data\n",
    "\n",
    "def pad_trunc(data, maxlen):\n",
    "    \"\"\" For a given dataset pad with zero vectors or truncate to maxlen \"\"\"\n",
    "    new_data = []\n",
    "\n",
    "    # Create a vector of 0's the length of our word vectors\n",
    "    zero_vector = []\n",
    "    for _ in range(len(data[0][0])):\n",
    "        zero_vector.append(0.0)\n",
    "\n",
    "    for sample in data:\n",
    " \n",
    "        if len(sample) > maxlen:\n",
    "            temp = sample[:maxlen]\n",
    "        elif len(sample) < maxlen:\n",
    "            temp = sample\n",
    "            additional_elems = maxlen - len(sample)\n",
    "            for _ in range(additional_elems):\n",
    "                temp.append(zero_vector)\n",
    "        else:\n",
    "            temp = sample\n",
    "        new_data.append(temp)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_name = \"galleryorrecordlabel\"\n",
    "dataset = pd.read_csv(\"data/\" + file_name + \".tsv\",sep = \"\\t\")\n",
    "# shuffle the DataFrame rows \n",
    "dataset = dataset.sample(frac = 1) \n",
    "features = tokenize_and_vectorize(dataset[\"text\"])\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, dataset[\"label\"], test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 50\n",
    "embedding_dims = 300    #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4986,), (2138,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(x_train).shape,np.array(x_test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pad_trunc(x_train, maxlen)\n",
    "x_test = pad_trunc(x_test, maxlen)\n",
    "x_train = np.reshape(x_train, (len(x_train), maxlen, embedding_dims))\n",
    "y_train = np.array(y_train)\n",
    "x_test = np.reshape(x_test, (len(x_test), maxlen, embedding_dims))\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4986, 50, 300), (2138, 50, 300))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(x_train).shape,np.array(x_test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32        # How many samples to show the net before backpropogating the error and updating the weights      # The width of the filters, actual filters will each be a matrix of weights of size: embedding_dims x kernel_size or 50 x 3 in our case\n",
    "num_neurons = 10     # Number of neurons in the plain feed forward net at the end of the chain\n",
    "epochs = 5    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 10)                12440     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 12,451\n",
      "Trainable params: 12,451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, SimpleRNN, LSTM\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(num_neurons, return_sequences=False, input_shape=(maxlen, embedding_dims)))\n",
    "model.add(Dropout(.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile('rmsprop', 'binary_crossentropy',  metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "156/156 [==============================] - 3s 16ms/step - loss: 0.4846 - accuracy: 0.7834 - val_loss: 0.3274 - val_accuracy: 0.8868\n",
      "Epoch 2/5\n",
      "156/156 [==============================] - 2s 14ms/step - loss: 0.3050 - accuracy: 0.9021 - val_loss: 0.2694 - val_accuracy: 0.9097\n",
      "Epoch 3/5\n",
      "156/156 [==============================] - 2s 14ms/step - loss: 0.2560 - accuracy: 0.9194 - val_loss: 0.2614 - val_accuracy: 0.9097\n",
      "Epoch 4/5\n",
      "156/156 [==============================] - 2s 14ms/step - loss: 0.2348 - accuracy: 0.9242 - val_loss: 0.2425 - val_accuracy: 0.9130\n",
      "Epoch 5/5\n",
      "156/156 [==============================] - 2s 14ms/step - loss: 0.2208 - accuracy: 0.9282 - val_loss: 0.2240 - val_accuracy: 0.9224\n",
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))\n",
    "model_structure = model.to_json()\n",
    "with open(\"simplernn_model2.json\", \"w\") as json_file:\n",
    "    json_file.write(model_structure)\n",
    "\n",
    "model.save_weights(\"simplernn_weights2.h5\")\n",
    "print('Model saved.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
