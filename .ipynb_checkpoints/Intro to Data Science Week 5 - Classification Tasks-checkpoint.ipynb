{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science - Week 5 Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data and plotting imports\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Machine learning imports\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import plot_tree\n",
    "!git clone https://github.com/Louismac/NLP-Public\n",
    "%cd NLP-Public"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://hackernoon.com/how-to-plot-a-decision-boundary-for-machine-learning-algorithms-in-python-3o1n3w07\n",
    "#A Function to plot decision boundarys\n",
    "def plot_decision(X,y,model,n_classes=2):\n",
    "    min1, max1 = X[:, 0].min()-1, X[:, 0].max()+1\n",
    "    min2, max2 = X[:, 1].min()-1, X[:, 1].max()+1\n",
    "    x1grid = np.arange(min1, max1, 0.1)\n",
    "    x2grid = np.arange(min2, max2, 0.1)\n",
    "    xx, yy = np.meshgrid(x1grid, x2grid)\n",
    "    r1, r2 = xx.flatten(), yy.flatten()\n",
    "    r1, r2 = r1.reshape((len(r1), 1)), r2.reshape((len(r2), 1))\n",
    "    grid = np.hstack((r1,r2))\n",
    "    model.fit(X, y)\n",
    "    yhat = model.predict(grid)\n",
    "    zz = yhat.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, zz, cmap='binary_r')\n",
    "    for class_value in range(n_classes):\n",
    "        row_ix = np.where(y == class_value)\n",
    "        plt.scatter(X[row_ix, 0], X[row_ix, 1], cmap='binary_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN\n",
    "\n",
    "## Experimenting\n",
    "\n",
    "The code below allows you to generate some data classes and plot a decision boundary \n",
    "\n",
    "1. Try different values of **k**\n",
    "\n",
    "\n",
    "2. Try moving around the ``centres`` and changing the ``standard deviation`` values, how does changing k work with different datasets?\n",
    "\n",
    "\n",
    "3. Add some more classes so its not a binary classification problem. You can do this by adding more sets of coordinates to the ``centres`` array. e.g. \n",
    "\n",
    "\n",
    "```\n",
    "centres = [\n",
    "    #x and y for class 1\n",
    "    [1,1],\n",
    "    #x and y for class 2\n",
    "    [3,3],\n",
    "    #x and y for class 3\n",
    "    [4,5]\n",
    "]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x and y coordinate for the centre of each class\n",
    "centres = [\n",
    "    #x and y for class 1\n",
    "    [1,1],\n",
    "    #x and y for class 2\n",
    "    [3,3]\n",
    "]\n",
    "#Controls the variation (deviation from the centre)\n",
    "standard_deviation = 1.4\n",
    "#number of nearest neighbours used for matching \n",
    "k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate data\n",
    "x, y = make_blobs(n_samples=500, centers=centres, random_state=1, cluster_std=standard_deviation)\n",
    "#Plot decision boundary \n",
    "plt.figure(figsize=(12,8))\n",
    "plot_decision(x, y, KNeighborsClassifier(n_neighbors=k), len(centres))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees \n",
    "\n",
    "## Experimenting\n",
    "\n",
    "The code below allows you to generate some data classes and plot a decision boundary. Compare behvaiour against the **KNN**\n",
    "\n",
    "1. Try different values of **max_depth**\n",
    "\n",
    "\n",
    "2. Try moving around the ``centres`` and changing the ``standard deviation`` values, how does changing max_depth work with different datasets?\n",
    "\n",
    "\n",
    "3. Add some more classes so its not a binary classification problem. You can do this by adding more sets of coordinates to the ``centres`` array.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x and y coordinate for the centre of each class\n",
    "centres = [\n",
    "    #x and y for class 1\n",
    "    [1,1],\n",
    "    #x and y for class 2\n",
    "    [3,3]\n",
    "]\n",
    "#Controls the variation (deviation from the centre)\n",
    "standard_deviation = 1.4\n",
    "#number of nearest neighbours used for matching \n",
    "max_depth = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate data\n",
    "x, y = make_blobs(n_samples=500, centers=centres, random_state=1, cluster_std=standard_deviation)\n",
    "#Plot decision boundary \n",
    "plt.figure(figsize=(12,8))\n",
    "plot_decision(x, y, DecisionTreeClassifier(max_depth=5), len(centres))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wine Dataset \n",
    "\n",
    "Now we're going to load in the wine dataset (from https://www.kaggle.com/rajyellow46/wine-quality).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/winequalityN.csv\")\n",
    "#Drop missing rows\n",
    "df = df.dropna()\n",
    "#Replace wines with \"type = white\" with 0\n",
    "df.loc[(df[\"type\"]==\"white\"), 'type'] = 0\n",
    "#Replace wines with \"type = red\" with 1\n",
    "df.loc[(df[\"type\"]==\"red\"), 'type'] = 1\n",
    "#Sort and get the first 1500 white wines and last 1500 red wines (balances classes)\n",
    "w = df.sort_values(\"type\")[0:1500]\n",
    "r = df.sort_values(\"type\")[-1500:]\n",
    "df = pd.concat([w,r])\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try different features \n",
    "\n",
    "1. Try combinations of two features from the 12 above, which is the most accurate? How does that effect the decision boundary?\n",
    "\n",
    "\n",
    "2. Try different max_depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Pick two features\n",
    "feature1 = \"chlorides\"\n",
    "feature2 = \"fixed acidity\"\n",
    "#Pick max_depth\n",
    "max_depth = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Train model, plot decision boundary \n",
    "x = df[[feature1,feature2]].values\n",
    "y = pd.to_numeric(df[\"type\"])\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.3, random_state=0)\n",
    "model = DecisionTreeClassifier(max_depth=max_depth)\n",
    "model.fit(x_train,y_train)\n",
    "#See if the model works\n",
    "y_pred = model.predict(x_test)\n",
    "num_incorrect = (y_test != y_pred).sum()\n",
    "total = y_test.shape[0]\n",
    "acc = (total - num_incorrect) / total * 100\n",
    "print(\"Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "#Set bounds to min and max values of features \n",
    "plt.xlim([np.min(x[:,0]),np.max(x[:,0])])\n",
    "plt.ylim([np.min(x[:,1]),np.max(x[:,1])])\n",
    "plot_decision(x, y, model, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting the tree\n",
    "\n",
    "See what the decision tree for your best feature pair looks like\n",
    "\n",
    "1. Look through the choices the tree makes. You may have to alter the `figsize` and `fontsize` for deeper trees.\n",
    "\n",
    "\n",
    "2. Look at splits the tree decided to make, can you see how it has balanced both large splits and low gini impurity scores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pick max_depth\n",
    "max_depth = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "my_tree = plot_tree(model, feature_names=[feature1, feature2],fontsize=11,class_names = [\"white\",\"red\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using more than 2 features \n",
    "\n",
    "Try using the whole feature set, inspect the tree and see how it favours some features over others \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pick max_depth\n",
    "max_depth = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pick all features for input\n",
    "x_labels = list(df.columns.values)\n",
    "x_labels.remove(\"type\")\n",
    "x = df[x_labels]\n",
    "#Fit model\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.3, random_state=0)\n",
    "model = DecisionTreeClassifier(max_depth=max_depth)\n",
    "model.fit(x_train,y_train)\n",
    "#See if the model works\n",
    "y_pred = model.predict(x_test)\n",
    "num_incorrect = (y_test != y_pred).sum()\n",
    "total = y_test.shape[0]\n",
    "acc = (total - num_incorrect) / total * 100\n",
    "print(\"Accuracy:\", acc)\n",
    "plt.figure(figsize=(15,8))\n",
    "my_plot = plot_tree(model, feature_names=x_labels,fontsize=11,class_names = [\"white\",\"red\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try with your own data!\n",
    "\n",
    "1. Pick some data you have found, or find some more! A place you might want to try is [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php) or [Kaggle](https://www.kaggle.com/datasets)\n",
    "\n",
    "\n",
    "2. Pick at least one categorical variable to be the class (`y`). \n",
    "\n",
    "    * You can make a continuous variable categorical using a threshold like below.  For example, here we make a new column called **college** which is **0 if less than 10 years in education** and **1 if 10 or more years in education**\n",
    "    \n",
    "    ``\n",
    "    split = 10\n",
    "    df.loc[(df[\"education.num\"]<split), 'college'] = 0\n",
    "    df.loc[(df[\"education.num\"]>=split), 'college'] = 1\n",
    "    ``\n",
    "\n",
    "\n",
    "3. Try some different models (you can even try and [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
