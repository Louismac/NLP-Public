{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2.2 Task: Text in the Wild\n",
    "\n",
    "This week we're going to find our own sets of documents online. This will sharpen our Python and regex skills as we try and overcome some of the issues with taking data from the real world. \n",
    "\n",
    "There are 3 types of text data that I will suggest and give you some code and pointers towards working with, but feel free to use any text you have lying around or can rustle up from the interweb. It just has to be in the right format. \n",
    "\n",
    "### The Format \n",
    "\n",
    "What we want to collect is sets of **documents** to compare. Each **document** is stored as a single string in a 1D array. \n",
    "\n",
    "```\n",
    "documents = [\n",
    "    document1,\n",
    "    document2,\n",
    "    document3\n",
    "    .....\n",
    "]\n",
    "```\n",
    "\n",
    "### Making Your Dataset\n",
    "\n",
    "I am going to suggest either getting books like I did, or film scripts, song lyrics. \n",
    "\n",
    "#### Books\n",
    "\n",
    "[Project Gutenberg](https://www.gutenberg.org/ebooks/) has loads of free eBooks. \n",
    "1. You can find some on here\n",
    "2. Just get the plain text versions of the book\n",
    "3. Make a new empty file in a text editor\n",
    "4. Copy/paste into a text editor and save as with .txt extension\n",
    "\n",
    "Then all we need to do is load them in as text files. You can choose to load in several books and treat them each as a separate document, or alternatively, you can load in one text file and try and split it into parts such as paragraphs or chapters using the regex skills you learnt last week.\n",
    "\n",
    "#### Song Lyrics\n",
    "\n",
    "1. You can find some song lyrics online\n",
    "2. Make a new empty file in a text editor\n",
    "3. Copy/paste into a text editor and save as with .txt extension\n",
    "\n",
    "As with books, you can find song lyrics can make text files with them and load each one in as separate text files. You can compare individual songs, lines in songs (split up the song using a regex), different artists, or different albums by the same artist. Just remember, what you want to end up with is an array where each document is a single string.\n",
    "\n",
    "#### Subtitle Files \n",
    "\n",
    "1. Find some subtitle files in `.srt` format online\n",
    "2. Save them on your computer and get the file paths\n",
    "3. Use the `pysrt` example code below to load them in\n",
    "\n",
    "As with books and songs, you can load each film or episode in as a separate document and compare them!\n",
    "\n",
    "### Loading in files\n",
    "\n",
    "Below we see code that takes an array of file paths and stores each one as a document. Alternatively, below it we take one text file and split it into documents using a regex.\n",
    "\n",
    "Remember, you can need to do some extra cleaning up of your data with regex depending on the format!\n",
    "\n",
    "#### Getting file paths\n",
    "\n",
    "If your text files are in the same folder as this notebook, you just need to use the filename as the path. This is called a **relative** path, as the path is _relative_ to the program you are trying to access it from (this notebook).\n",
    "\n",
    "For example, you would just need `hacking.txt`. \n",
    "\n",
    "\n",
    "If you have the text files somewhere else on your computer, you'll need an **absolute** path. This means you need to give the whole path starting from the root of your file system. \n",
    "\n",
    "* For Mac OSX, this is often `/Users/[USERNAME]/Documents/...`\n",
    "    - You can find a file path by\n",
    "     1. Opening a new Terminal window \n",
    "     2. Dragging and dropping a file into it\n",
    "     \n",
    "* For Windows, it will probably look like `C:\\Documents\\...` (the slashes go the other way!)\n",
    "    - You can find a file path using any other [these](https://www.wikihow.com/Find-a-File%27s-Path-on-Windows) methods. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/louismccallum/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Import all necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import re\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "import nltk\n",
    "nltk.download(\"wordnet\")\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cosine\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text files (Books, songs etc...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Each txt file is a separate document\n",
    "#Put your file paths in here\n",
    "urls = [\n",
    "    \"path1.txt\",\"path2.txt\",\"path3.txt\"\n",
    "]\n",
    "documents = []\n",
    "for url in urls:\n",
    "    #Open file\n",
    "    fs = open(url, 'r') \n",
    "    #Read in text as string\n",
    "    doc = fs.read() \n",
    "    #Add string to array of documents\n",
    "    documents = documents + [doc]\n",
    "documents = np.array(documents)\n",
    "print(documents.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split single txt file into documents with a regex\n",
    "url = \"path1.txt\"\n",
    "#Open file\n",
    "fs = open(url, 'r') \n",
    "#Read in text as string\n",
    "doc = fs.read() \n",
    "#Split based on your own regex\n",
    "documents = np.array(re.split(r'\\s\\s\\s\\s\\s\\sChapter+', doc))\n",
    "print(documents.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtitle files\n",
    "\n",
    "Here you can try and load in subtitle files in the `.srt` format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install pysrt package\n",
    "!pip install pysrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "#getting your own subs\n",
    "import pysrt\n",
    "#Put your file paths in here\n",
    "urls = [\n",
    "    \"path1.txt\",\"path2.txt\",\"path3.txt\"\n",
    "]\n",
    "documents = []\n",
    "for url in urls:\n",
    "    #Load in the susbitle file and parse\n",
    "    subs = pysrt.open(url)\n",
    "    #Connect together linebreaks\n",
    "    subs = [l.text.replace(\"\\n\", \" \") for l in subs]\n",
    "    #Join into one string\n",
    "    subs = \" \".join(subs)\n",
    "    #Add string to array of documents \n",
    "    documents = documents + [subs]\n",
    "documents = np.array(documents)\n",
    "print(documents.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing your text\n",
    "\n",
    "Now we have a set of documents in an array, we can start turning them into numerical representations.\n",
    "\n",
    "Luckily, the `sklearn` library has built in functions for us to use when we want to make **word vectors**. We can either use the `Count Vectorizer` to make a bag of words, or `TfidfVectorizer` to get TF/IDF vectors. \n",
    "\n",
    "We also define our own **tokeniser**. The function `my_tokeniser` in called on every document, and returns an array of tokens. We can edit this function to try different stemming, lemmatisation, capitalisation, n-grams all of which we have seen in the Week 2.2 lecture notebook and can copy across!\n",
    "\n",
    "Underneath is code for seeing the highest scoring words for each document, as well as viewing the similarities against each other. \n",
    "\n",
    "Try different approaches to tokenising and compare Bag of Words against TF/IDF on your documents, what do you find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Called once for each document\n",
    "def my_tokeniser(doc):\n",
    "    #Split on spaces\n",
    "    tokens = re.split(r'[-\\s.,;!?]+', doc)\n",
    "    processed = []\n",
    "    for t in tokens:\n",
    "        #Lemmatise and make lowercase\n",
    "        t = lem.lemmatize(t.lower())\n",
    "        #Remove stop words\n",
    "        if not t in stop_words.ENGLISH_STOP_WORDS:\n",
    "            processed = processed + [t]\n",
    "    #Return an array of tokens for that document\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the CountVectorizer to get a bag of words using a custom tokeniser\n",
    "vectoriser = CountVectorizer(tokenizer=my_tokeniser)\n",
    "vectorised = vectoriser.fit_transform(documents)\n",
    "print(vectorised.todense().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the TFIDF Vectorizer to get TFIDF vectors with custom tokeniser\n",
    "vectoriser = TfidfVectorizer(tokenizer=my_tokeniser)\n",
    "vectorised = vectoriser.fit_transform(documents)\n",
    "print(vectorised.todense().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing the highest scoring words per document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store in a dataframe and sort\n",
    "num_words = 10\n",
    "#Use the vocab as the column names\n",
    "vocab = vectoriser.get_feature_names()\n",
    "data = pd.DataFrame(vectorised.todense(), columns = vocab)\n",
    "for i in range(len(vectorised.todense())):\n",
    "    print(\"doc\", i)\n",
    "    print(data.iloc[i].sort_values(ascending = False).head(num_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing the similarities between documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Convert to array \n",
    "vector_array = vectorised.todense()\n",
    "#Find similarities\n",
    "result = cosine(vector_array)\n",
    "#Put the result in a dataframe and \n",
    "df = pd.DataFrame(result)\n",
    "#Show with heatmap style gradients\n",
    "df.style.background_gradient(cmap='Greens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
