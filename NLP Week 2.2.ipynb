{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2.2 Representing Documents as Numbers\n",
    "## NLP for the Creative Industries \n",
    "### Louis McCallum*\n",
    "#### 2020\n",
    "##### *who, after pushing it to its limits, has found out there are 5 levels of subheading in Mark-up\n",
    "###### not 6, which the same as 5\n",
    "####### Or 7, then it just turns into text and hashtags\n",
    "\n",
    "Hopefully, you have followed the Git tutorial and have managed to update your repo and pull in this new notebook! \n",
    "\n",
    "If we want to find take a document or set of documents, and use them with machine learning techniques, or other mathematical operation to uncover some new information abou them, we can't just use the text and characters. We need a new representation, and that need to be numerical. This week we will be taking our first steps to representing collections of documents as numbers. \n",
    "\n",
    "- Tokenisation \n",
    "- Bags of Words \n",
    "- n-Grams\n",
    "- TF/IDF\n",
    "- Distances between documents \n",
    "\n",
    "### Building a Vocabulary \n",
    "\n",
    "The first step to getting a new, better representation of a text document is splitting it up into its consistuent parts. We call these **tokens** and deciding _what a token is_ in important choice. \n",
    "\n",
    "### Tokenisation \n",
    "We have already done some basic tokenisation in Week 1 using the `str.split()` method. Here we took a long string (multiple words) and split it into separate words (or \"tokens\") based on spaces. \n",
    "\n",
    "Does this seem sensible? Does this capture every thing that we would consider a separate word in the document? \n",
    "\n",
    "What about `isn't`? Is this one token (`isn't`), or two tokens (`is` and `not`)? Or `taxi cab`? Is that two tokens, do we care that `taxi` and `cab` are both used? Do we need this as a separate concept from `Uber`, `limo`, `Hackney Carriage` or `car`?. If we take it as one, do we miss out on other combinations like `black cab` or `taxi driver`? What about punctuation?\n",
    "\n",
    "Just using `str.split()` works reasonably well on the sentence below. However, there are some issue with punctuation as ideally, the brackets and the exclaimation mark would also be separate tokens. \n",
    "\n",
    "After we have split our sentence into tokens, we can then create a **vocabulary**, which contains every unique token in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#String split\n",
    "sentence = \"I like to think (it has to be!) of a cybernetic ecology where we are free of our labors and joined back to nature\"\n",
    "tokenised = sentence.split()\n",
    "print(tokenised)\n",
    "#Get the unique tokens (removes duplicates)\n",
    "vocab = np.unique(tokenised)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectors and Matrices\n",
    "\n",
    "A quick point on terminology before we move forwards. The arrays we have seen up until this point have mainly been 1  dimensional, that is all the items in the list are just single objects like numbers or strings. But, it is possible to have lists in multiple dimensions and for the time being we will just move to 2. \n",
    "\n",
    "It can help to think of a 1D array as a queue, or a shopping list. There are only two directions (backwards and forwards) and you only need **1 index** to access an item in it. You can think of 2D array is a grid, so more like a chess board. You can move in 4 directions (forwads, backwards, left and right) and you need **2 indexes** to access any item.\n",
    "\n",
    "Technically, in a 2D array, each item of the outer array is also another 1D array.\n",
    "\n",
    "Taking from mathematics, these 1D lists are often called **vectors** and 2D arrays are called **matrices** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#a is a matrix\n",
    "a = np.array([[1,2],[3,4]])\n",
    "print(\"a matrix\")\n",
    "print(a)\n",
    "#Get the first row (a vector)\n",
    "print(\"a vector\", a[0])\n",
    "#Get a specific item [row, col]\n",
    "print(a[1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Loops\n",
    "\n",
    "Below we use a **for loop** to do this. This is something we use when we want to do a repeated action for a given number of times. The code below shows the standard structure. It tells us to take every item in `array` **in order**, and store it in `i`. The code underneath dictates what repeated actions we do with i each time and **must** be indented with a tab, otherwise Python will complain. This can be a single line code, or multiple lines. Every line that is indented will be included in the loop and executed each time.\n",
    "\n",
    "`for i in array:\n",
    "    do_something_with_i \n",
    "#end of loop\n",
    "`\n",
    "    \n",
    "`for i in array:\n",
    "     do_something_with_i\n",
    "     do_something_else_with_i\n",
    "     do_another_thing\n",
    "#end of loop\n",
    "`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For loop prints out every item in turn\n",
    "a = [1,2,3,4,5,10,15]\n",
    "for i in a:\n",
    "    print(i)\n",
    "print(\"end of for loop 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use range to get a sequence of numbers from 0->length of array\n",
    "indexes = range(len(a))\n",
    "for i in indexes:\n",
    "    print(i,\":\",a[i])\n",
    "print(\"end of for loop 2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can have multiple lines of code in the loop\n",
    "#They are all indented \n",
    "for i in a:\n",
    "    print(i)\n",
    "    print(i + 2)\n",
    "    print(\"this happens every time\")\n",
    "print(\"end of for loop 3, this only happens once\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding\n",
    "\n",
    "Now that we have derived full vocabulary (if not exactly perfect yet) for the sentence, we can represent is a set of numbers. We take each token in the vocabulary and assign it an index, then for every token in the sentence, we create a vector that is all 0s, apart from a single 1 at the index taken from the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into tokens based on spaces\n",
    "tokenised = sentence.split()\n",
    "#Get the unique tokens\n",
    "vocab = np.unique(tokenised)\n",
    "print(vocab)\n",
    "#Make a matrix of zeros using the lengths of the separated sentence and vocab\n",
    "one_hot = np.zeros((len(tokenised), len(vocab)))\n",
    "#Go through the separated sentence and set the appropriate item to 1\n",
    "for i in range(len(tokenised)):\n",
    "    #Get the word\n",
    "    word = tokenised[i]\n",
    "    #find the index of the word in the vocab\n",
    "    match = np.where(vocab == word)\n",
    "    #Set it to 1 (hot)\n",
    "    one_hot[i, match] = 1\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This **one-hot encoding** doesn't lose any information, but its a lot of numbers for a small amount of information. This being said, it is also super **sparse**, which just means there are lots of 0s, and there are actually lots of techinques for really efficiently storing sparse data. \n",
    "\n",
    "We've successfully represented our sentence as a maxtrix of numbers, which we can use with various mathematical techniques moving forwards. \n",
    "\n",
    "### Bag of Words\n",
    "\n",
    "Using **one-hot encoding**, we have a **vector** the length of the vocabulary for every word.\n",
    "\n",
    "This can quickly get out of hand with longer documents and bigger vocabularies.\n",
    "\n",
    "One improvement we can make to this representation is to simply count up every occurence of each word in the vocabulary for each document, and then store this count for each word in the vocabulary. \n",
    "\n",
    "This means we only have one **word frequency vector** for each document, rather than a one-hot encoding vector for each word. If we had multiple documents (or sentences), we could make a **word frequency vector** for each one and store them as a **matrix** (2D array).\n",
    "\n",
    "The dictionary that we get out of the Counter object is an efficient storage method, as absent words are just ignored.\n",
    "\n",
    "Even though this a big compression of the data, this approach actually ends up not losing much of the meaning of the document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use a Counter to create a Bag of Words (word-frequency vector)\n",
    "from collections import Counter\n",
    "bow = Counter(tokenised)\n",
    "bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at Books\n",
    "Here we have a novel from [Project Gutenberg](https://gutenberg.org/ebooks/). Its about hacking is and copyright free. Lets see what we can find out about it.\n",
    "\n",
    "What can we find out about each chapter by counting the amount of times a word appears in each? Are any similar to each other? How can we adjust our vocabulary to help us out? First we're going to look at the book as a whole.\n",
    "\n",
    "We're going to use a number of techniques to try and tweak our vocabulary so that it contains the most information for when we start using this bag of words in tasks like topic modelling or classification. \n",
    "\n",
    "This means we want to count things that have the same meaning as the same token, but we also don't want to throw away any information that might help our processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "fs = open('hacking.txt', 'r') \n",
    "book = fs.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tokens is a 1D array\n",
    "tokens = book.split()\n",
    "#Get the unique tokens (our vocabulary)\n",
    "vocab = np.unique(tokens)\n",
    "print(\"total words:\",len(tokens), \"unique words:\", len(vocab))\n",
    "#Create a Bag of Words using a Counter\n",
    "Counter(tokens).most_common(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find all words that have a . or , in them. \n",
    "ctr = 0;\n",
    "for word in vocab:\n",
    "    if len(re.findall(r'[\\.,;!?]$', word)) > 0:\n",
    "        if word[:-1] in vocab:\n",
    "            ctr = ctr + 1\n",
    "print(ctr,\" duplicate words ending in [.,;!?] out of \", len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing punctuation duplicates\n",
    "We can see lots of tokens have trailing punctutation, and a good few of these will also exist in without the punctuation. This duplication is bad for us!\n",
    "\n",
    "We would want these to be the same token so we can use a **regex** to replace it. The regex below splits on whitespace (represented by `\\s`) or punctuation that appears at least once (using this plus notation `+`). We immediately see the size of the vocabulary drops by about 25%, showing there was loads of duplication in our bag of words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Use a regex to split based on space AND punctuation\n",
    "tokens = re.split(r'[-\\s.,;!?]+', book)\n",
    "vocab = np.unique(tokens)\n",
    "print(\"unique words\", vocab.shape)\n",
    "#Counter(tokens).most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words\n",
    "We can see that the commonly occurring words don't tell us much about this specific book. Traditionally, in NLP it has been useful to remove words that occur commonly. They don't tell us very much about each document because they are contained in almost all the documents. These are known as **stop words**. \n",
    "\n",
    "In contemporary NLP we often don't actually remove stop words because we have the computing power to deal with the extra vocab size and any information we throw away can effect performance, especially in deep learning, and especially when we start to look at context of sequences of words. \n",
    "\n",
    "Here we see a list from the sklearn library (each library has its own list of stop words). \n",
    "\n",
    "We'll just see how removing stop words from our bag effects what we can see. Although our vocabulary size is basically the same (so we're not saving much in effiency), the list most common words are much more informative and tells us more about the specific book we're looking at. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install library if not already installed\n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import stop_words\n",
    "stop_tokens = []\n",
    "for t in tokens:\n",
    "    if not t in stop_words.ENGLISH_STOP_WORDS:\n",
    "        stop_tokens = stop_tokens + [t]\n",
    "stop_vocab = np.unique(stop_tokens)\n",
    "print(\"unique words\", stop_vocab.shape)\n",
    "Counter(stop_tokens).most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "Stemming attempt to remove suffixes from words that contain the same base. This reduces variation and can help when we reduce the documents into a more distilled form (like a bag of words). \n",
    "\n",
    "- hacking, hackers, hacked, hacks\n",
    "- computer, computing, computers\n",
    "\n",
    "Depending on our task, its probably the case that we only really care about knowing if **any** of these words appear, not whether they each appear individually. For example, I might be doing a search for paragraphs about hacking and it may be that I would miss out on key documents otherwise if I only searched for one of the words. \n",
    "\n",
    "Stemming can be quite a challenging task however. If we want to combine pluralisations, for example, we can't just remove the \"s\" from the end of all nouns, what about \n",
    "\n",
    "- grass (not a plural)\n",
    "- mice, octopi (plural, no s)\n",
    "- geniuses (plural, es)\n",
    "\n",
    "We're going to use the built in stemmer in the NLTK library. This reduces our vocabulary in the hacking book dramatically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install the nltk library \n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "word_list = ['feet', 'foot', 'foots', 'footing']\n",
    "for word in word_list:\n",
    "    print(word, \"->\", stemmer.stem(word))\n",
    "#Doesn't always work\n",
    "word_list = ['organise','organises','organised','organisation',\"organs\",\"organ\",\"organic\"]\n",
    "for word in word_list:\n",
    "    print(word, \"->\", stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Looking at our hacking book\n",
    "stem_tokens = []\n",
    "for t in stop_tokens:\n",
    "    stem_tokens = stem_tokens + [stemmer.stem(t)]\n",
    "stem_vocab = np.unique(stem_tokens)\n",
    "print(\"unique words\", stem_vocab.shape)\n",
    "Counter(stem_tokens).most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatisation \n",
    "Lemmatisation is a technique similar to stemming, apart from it attempts to find similar meanings, as opposed to just similar roots. Like with all these _normalisation_ techniques, reducing your vocabulary will reduce precision but may make your model bettter at generalising and more efficient.\n",
    "\n",
    "For example, lemmatisation would be able to separate **dogged** and **dog**, which have quite different meanings but would get combined by a stemmer. \n",
    "\n",
    "Below we use the WordNetLemmatizer from the NLTK library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"wordnet\")\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lem.lemmatize(\"dogged\"))\n",
    "print(lem.lemmatize(\"dog\"))\n",
    "print(stemmer.stem(\"dogged\"))\n",
    "print(stemmer.stem(\"dog\"))\n",
    "print(lem.lemmatize(\"better\", pos =\"a\"))\n",
    "print(lem.lemmatize(\"better\", pos = \"v\"))\n",
    "print(lem.lemmatize(\"betterment\", pos = \"n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Looking at our hacking book\n",
    "lem_tokens = []\n",
    "for t in stop_tokens:\n",
    "    lem_tokens = lem_tokens + [lem.lemmatize(t)]\n",
    "lem_vocab = np.unique(lem_tokens)\n",
    "print(\"unique words\", lem_vocab.shape)\n",
    "Counter(lem_tokens).most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capitalisation\n",
    "Whilst it may be tempting to just lower case every token with the belief that words all have the same meaning regardless of case. However, it may actually be that if something is in ALL CAPS it conveys some meaning. Or if a word is at the start of sentence, that has importance. \n",
    "\n",
    "For example \n",
    "\n",
    "- John liked to help\n",
    "- John screamed HELP HELP HELP\n",
    "\n",
    "Or if one book contained lots of capitalised nouns (like cities and countries), it might tell you it was about Geography.\n",
    "\n",
    "Some libraries actually account for this by lower casing everything, then having a token which indicates a start of capitilising as well as one that signifies the end of capitalising. This allows the best of both worlds. Of course, this only works if your model or vocabulary is able to take sequence and context into account. Like for example...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lower_tokens = []\n",
    "for word in lem_tokens:\n",
    "    lower_tokens = lower_tokens + [word.lower()]\n",
    "lower_vocab = np.unique(lower_tokens)\n",
    "print(\"unique words\", lower_vocab.shape)\n",
    "bow = Counter(lower_tokens)\n",
    "bow.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-Grams\n",
    "When we do Bag of Words, what we gain in effiency and generalisation, we lose in context. We can gain lots by including combinations of words as tokens, and these are known as **n-grams**. For example, if our book was a tale about an butcher / computer programmer / hacky-sack apologist, we could could see the term `hacking` in a bunch of contexts. If we don't look at the subsequent token, we lose this information.\n",
    "\n",
    " - hacking meat\n",
    " - hacking computers\n",
    " - hacking sack\n",
    " \n",
    "On a more realistic note, I see that `phone` and `network` appear almost exactly the same amount of times, and I'd wager this is because they follow one another and using multiword tokens will cpature this connection. Also consider **not**. A newspaper article that said someone was `not guilty` and `not under arrest` and `not a bad chap`, could be summarised with `guilty`, `arrest` and `bad` with just single word tokens.\n",
    " \n",
    "A 2-gram makes a token out of very pair of subsequent words, and a 3-gram out of every 3 etc.\n",
    "\n",
    "What we see when we apply this to our book is that there is a massive explosion in the number of tokens. Which makes sense, as we're counting seqeuences and its likely that words will appear in more than sequence, and that exact matching sequences will appear less times that single words. \n",
    "\n",
    "But we also see some new things, for example, `australian` by itself didnt make the top 50, but when combined with to make `australian hacker`, it becomes much more common in respect to the whole document. This quickly lets us know perhaps of all the hackers in the book, an australian one is the most prominent. Or if there's only one hacker, they're most prominent feature is their australian-ness. \n",
    "\n",
    "And although '`computer` is very regular in the single word list, we get more context with 2-grams such as `computer undergroud` and `computer crime`.\n",
    "\n",
    "Some of thes combinations will be super rare and its common to drop any n-grams that don't appear often from your vocabulary. When your feature vector (the size of the bag of words) gets bigger than the number of documents you have, you start to get problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "ngram_tokens = []\n",
    "#Loop through all the tokens\n",
    "for i in range(len(lower_tokens)-n):\n",
    "    word = lower_tokens[i]\n",
    "    #Stich together with subsequent tokens into one string\n",
    "    for j in range(n - 1):\n",
    "        word = word + \" \" + lower_tokens[i + 1 + j]\n",
    "    #Add to list\n",
    "    ngram_tokens = ngram_tokens + [word]\n",
    "ngram_vocab = np.unique(ngram_tokens)\n",
    "print(\"unique words\", ngram_vocab.shape)\n",
    "Counter(ngram_tokens).most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF/IDF\n",
    "\n",
    "Up until this point, we've seen how counting words, and looking at the most frequent can gives us some insight into a single document. If we want to start comparing documents with more certainty, or getting smarter about our representations, we can try and get a set of numbers for each documents that not only represents **word frequency**, but also **word importance**. \n",
    "\n",
    "What we will end up with is a measurement called **TF/IDF** or **T**erm **F**requency x **I**nverse **D**ocument **F**requency. \n",
    "\n",
    "### TF\n",
    "\n",
    "**TF** stands for **term frequency** and we've been using it a lot already in our Bags of Words. By itself, it tells us how many times a particular term appears in a document. Can we do better?\n",
    "\n",
    "Consider our book and some of its most common words\n",
    "\n",
    "- computer \n",
    "- hacking\n",
    "- security \n",
    "- police\n",
    "- network\n",
    "\n",
    "These words seem to represent key topics of the book quite well. However, what about **mother**? This appears 113  times across the book, out of a vocabulary of  approx. 13,000 words. Compare this to a WhatsApp conversation that me and my sister had about our family Christmas that has the word **mother** 5 times in with a vocabulary of about 50 words. When we compare just **term frequency**, it seems like the hacking book is far, far more (~20 times) about mothers than this text message chain. But thats not really the case. \n",
    "\n",
    "We use **normalised term frequency** to account for this, where the length of the document is used alongside the count to adjust for this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide term frequency by total number of unique words (vocab size)\n",
    "book_tf = bow[\"mother\"] / len(lower_vocab)\n",
    "text_msg_tf = 5 / 50\n",
    "#Much bigger normalised term frequency for text msgs\n",
    "print(book_tf, text_msg_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDF\n",
    "\n",
    "**IDF** stands for **I**nverse **D**ocument **F**requency and it tells us how important a word is in a particular document in comparison to the rest of the corpus. Up until this point we've been considering the book as one big document, but now we're going to take each chapter on its own, to see if we can see if we can highlight differences between them.\n",
    "\n",
    "We can see below that most chapters have the terms **computer** and **hacker** featuring pretty heavily. The **IDF** is the ratio of all documents in comparison to how many documents the term appears in. It tells us how surprising is it that this word appeared here, given what we know about all the documents. \n",
    "\n",
    "First, we use a **regex** to split it into chapters, as there is a recognisable formatting to this. This means our corpus is the whole novel, with each chapter considered a new document and we store the whole thing as a 1D array. Each item in the array is a string containing a chapter's worth of text.\n",
    "\n",
    "### Getting the TF/IDF Vector for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First get the Bag of Words for each chapter \n",
    "bow_chapters = []\n",
    "#Split on Chapters\n",
    "book = re.split(r'\\s\\s\\s\\s\\s\\sChapter+', book)\n",
    "for chapter in book:\n",
    "    #Split on spaces and punctuation\n",
    "    tokens = re.split(r'[-\\s.,;!?]+', chapter)\n",
    "    processed = []\n",
    "    for t in tokens:\n",
    "        #Lemmatise and make lowercase\n",
    "        t = lem.lemmatize(t.lower())\n",
    "        if not t in stop_words.ENGLISH_STOP_WORDS:\n",
    "            processed = processed + [t]\n",
    "    bow = Counter(processed)\n",
    "    print(\"****new chapter****\")\n",
    "    print(bow.most_common(10))\n",
    "    bow_chapters = bow_chapters + [bow]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all the vocabulary across all chapters\n",
    "vocab = []\n",
    "for chapter in bow_chapters:\n",
    "    vocab = vocab + list(chapter.keys())\n",
    "vocab = np.unique(vocab)\n",
    "\n",
    "tf_chapters = []\n",
    "num_docs = len(book)\n",
    "vocab_size = len(vocab)\n",
    "docs_containing_word = np.zeros(len(vocab))\n",
    "#Go through each chapter\n",
    "for i in range(num_docs):\n",
    "    tf = []\n",
    "    #Go through each term in the vocab\n",
    "    for j, term in enumerate(vocab):\n",
    "        count = 0;\n",
    "        #If the term is present in the chapter, collect data\n",
    "        if bow_chapters[i][term]:\n",
    "            #Save the normalised TF\n",
    "            count = bow_chapters[i][term] / vocab_size\n",
    "            #Note that this word appeared in this doc (document frequency)\n",
    "            docs_containing_word[j] = docs_containing_word[j] + 1\n",
    "        #Add to array of terms\n",
    "        tf = tf + [count]\n",
    "    #Add to array of chapters\n",
    "    tf_chapters = tf_chapters + [tf]\n",
    "\n",
    "tf_chapters = np.array(tf_chapters)\n",
    "print(tf_chapters.shape)\n",
    "tfidf = np.zeros(tf_chapters.shape)\n",
    "#For every chapter\n",
    "for i in range(num_docs):\n",
    "    #For every term\n",
    "    for j in range(vocab_size):\n",
    "        #Calculate TF/IDF\n",
    "        tfidf[i, j] = tf_chapters[i, j] * np.log((num_docs / docs_containing_word[j]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the highest TF/IDF values\n",
    "\n",
    "Now we which words are important to each chapter. Interestingly we've lost all of the words like `computer` and `hacking`, because they're surprising or indicative of that chapter, given the whole corpus. These words are the words that tell us the most about each chapter.\n",
    "\n",
    "It seems likes names (of people and of viruses?) are important distinctions between chapters. \n",
    "\n",
    "We also did lemmatisation instead of stemming and often have the same word, and its possesive version in a chapter (`anthrax` and `anthrax's`). Maybe stemming would be better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Store in a dataframe and sort\n",
    "import pandas as pd\n",
    "#Use the vocab as the column names\n",
    "data = pd.DataFrame(tfidf, columns = vocab)\n",
    "for i in range(len(tfidf)):\n",
    "    print(\"chapter\", i)\n",
    "    print(data.iloc[i].sort_values(ascending = False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maths With Word Vectors\n",
    "So what we have now is a **vector** for each document (in our case, each document is a chapter). This vector represents something about the text in that chapter based on the frequencies that words occur, and how that relates to the corpus as a whole. \n",
    "\n",
    "We can use these vectors calculate how similar two documents by calculating the distance between them. Our vectors are currently >10,000. This means this is the _dimensionality_ of our vector. We can actually use similar maths that we would use to work out the distance between 2 points in 2 dimensional space. And its much easier to visualise how that works!\n",
    "\n",
    "Two methods often used are **Manhattan distance** and **Euclidean** distance, but we tend to use something else for TF/IDF vectors.\n",
    "\n",
    "### Cosine Distance\n",
    "\n",
    "What we actually want to use is something called the **cosine distance**, which essentially tells how much the two vectors are pointing in the same direction. The results go from -1 to 1, where 1 is exactly the same, 0 is nothing in common and -1 is **anti-similar**. However, this never happens for TFIDF vectors, because word counts can never be negative!\n",
    "\n",
    "### Topic Modelling \n",
    "\n",
    "What we already see is that we can begin to group documents together by how similar they are. Next week Rebecca will teach you some more advanced methods for taking this idea further. \n",
    "\n",
    "Interestingly, the first chapter seems to be the most different from the rest, and I think that isn't a Chapter per se, but the preface. Also, consecutive chapters tend to be the most simliar to each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the cosine similarity method from sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cosine\n",
    "result = cosine(tfidf)\n",
    "#Put the result in a dataframe and \n",
    "df = pd.DataFrame(result)\n",
    "#Show with heatmap style gradients\n",
    "df.style.background_gradient(cmap='Greens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round up\n",
    "\n",
    "So here we've seen how we can take a bunch of text documents, and represent them as a collection of numerical vectors. This representation means we can start to use basic maths to compare documents, and we'll see in future weeks how this representation is the building block of tonnes of other more complex processing, including machine learning models. \n",
    "\n",
    "When getting this new representation, depending on our task, we want something that \n",
    "\n",
    "- Is as few dimensions as possible **BUT** maintains as much information as possible. Its always a balance and will depend on your goals, modelling approach and dataset.\n",
    "- Correctly captures the important points of document \n",
    "- Highlights the differences between documents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
